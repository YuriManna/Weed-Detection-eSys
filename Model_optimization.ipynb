{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39c17bc",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0178aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.3 is exactly one major version older than the runtime version 6.30.2 at onnx/onnx-ml.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.3 is exactly one major version older than the runtime version 6.30.2 at onnx/onnx-operators-ml.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.3 is exactly one major version older than the runtime version 6.30.2 at onnx/onnx-data.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model summary: 169 layers, 25,857,478 parameters, 0 gradients, 79.1 GFLOPs\n",
      "model.model.0.conv.weight: torch.float32\n",
      "model.model.0.bn.weight: torch.float32\n",
      "model.model.0.bn.bias: torch.float32\n",
      "model.model.1.conv.weight: torch.float32\n",
      "model.model.1.bn.weight: torch.float32\n",
      "model.model.1.bn.bias: torch.float32\n",
      "model.model.2.cv1.conv.weight: torch.float32\n",
      "model.model.2.cv1.bn.weight: torch.float32\n",
      "model.model.2.cv1.bn.bias: torch.float32\n",
      "model.model.2.cv2.conv.weight: torch.float32\n",
      "model.model.2.cv2.bn.weight: torch.float32\n",
      "model.model.2.cv2.bn.bias: torch.float32\n",
      "model.model.2.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.2.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.2.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.2.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.2.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.2.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.2.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.2.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.2.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.2.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.2.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.2.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.3.conv.weight: torch.float32\n",
      "model.model.3.bn.weight: torch.float32\n",
      "model.model.3.bn.bias: torch.float32\n",
      "model.model.4.cv1.conv.weight: torch.float32\n",
      "model.model.4.cv1.bn.weight: torch.float32\n",
      "model.model.4.cv1.bn.bias: torch.float32\n",
      "model.model.4.cv2.conv.weight: torch.float32\n",
      "model.model.4.cv2.bn.weight: torch.float32\n",
      "model.model.4.cv2.bn.bias: torch.float32\n",
      "model.model.4.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.4.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.4.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.4.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.4.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.4.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.4.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.4.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.4.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.4.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.4.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.4.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.4.m.2.cv1.conv.weight: torch.float32\n",
      "model.model.4.m.2.cv1.bn.weight: torch.float32\n",
      "model.model.4.m.2.cv1.bn.bias: torch.float32\n",
      "model.model.4.m.2.cv2.conv.weight: torch.float32\n",
      "model.model.4.m.2.cv2.bn.weight: torch.float32\n",
      "model.model.4.m.2.cv2.bn.bias: torch.float32\n",
      "model.model.4.m.3.cv1.conv.weight: torch.float32\n",
      "model.model.4.m.3.cv1.bn.weight: torch.float32\n",
      "model.model.4.m.3.cv1.bn.bias: torch.float32\n",
      "model.model.4.m.3.cv2.conv.weight: torch.float32\n",
      "model.model.4.m.3.cv2.bn.weight: torch.float32\n",
      "model.model.4.m.3.cv2.bn.bias: torch.float32\n",
      "model.model.5.conv.weight: torch.float32\n",
      "model.model.5.bn.weight: torch.float32\n",
      "model.model.5.bn.bias: torch.float32\n",
      "model.model.6.cv1.conv.weight: torch.float32\n",
      "model.model.6.cv1.bn.weight: torch.float32\n",
      "model.model.6.cv1.bn.bias: torch.float32\n",
      "model.model.6.cv2.conv.weight: torch.float32\n",
      "model.model.6.cv2.bn.weight: torch.float32\n",
      "model.model.6.cv2.bn.bias: torch.float32\n",
      "model.model.6.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.6.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.6.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.6.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.6.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.6.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.6.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.6.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.6.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.6.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.6.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.6.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.6.m.2.cv1.conv.weight: torch.float32\n",
      "model.model.6.m.2.cv1.bn.weight: torch.float32\n",
      "model.model.6.m.2.cv1.bn.bias: torch.float32\n",
      "model.model.6.m.2.cv2.conv.weight: torch.float32\n",
      "model.model.6.m.2.cv2.bn.weight: torch.float32\n",
      "model.model.6.m.2.cv2.bn.bias: torch.float32\n",
      "model.model.6.m.3.cv1.conv.weight: torch.float32\n",
      "model.model.6.m.3.cv1.bn.weight: torch.float32\n",
      "model.model.6.m.3.cv1.bn.bias: torch.float32\n",
      "model.model.6.m.3.cv2.conv.weight: torch.float32\n",
      "model.model.6.m.3.cv2.bn.weight: torch.float32\n",
      "model.model.6.m.3.cv2.bn.bias: torch.float32\n",
      "model.model.7.conv.weight: torch.float32\n",
      "model.model.7.bn.weight: torch.float32\n",
      "model.model.7.bn.bias: torch.float32\n",
      "model.model.8.cv1.conv.weight: torch.float32\n",
      "model.model.8.cv1.bn.weight: torch.float32\n",
      "model.model.8.cv1.bn.bias: torch.float32\n",
      "model.model.8.cv2.conv.weight: torch.float32\n",
      "model.model.8.cv2.bn.weight: torch.float32\n",
      "model.model.8.cv2.bn.bias: torch.float32\n",
      "model.model.8.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.8.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.8.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.8.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.8.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.8.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.8.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.8.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.8.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.8.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.8.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.8.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.9.cv1.conv.weight: torch.float32\n",
      "model.model.9.cv1.bn.weight: torch.float32\n",
      "model.model.9.cv1.bn.bias: torch.float32\n",
      "model.model.9.cv2.conv.weight: torch.float32\n",
      "model.model.9.cv2.bn.weight: torch.float32\n",
      "model.model.9.cv2.bn.bias: torch.float32\n",
      "model.model.12.cv1.conv.weight: torch.float32\n",
      "model.model.12.cv1.bn.weight: torch.float32\n",
      "model.model.12.cv1.bn.bias: torch.float32\n",
      "model.model.12.cv2.conv.weight: torch.float32\n",
      "model.model.12.cv2.bn.weight: torch.float32\n",
      "model.model.12.cv2.bn.bias: torch.float32\n",
      "model.model.12.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.12.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.12.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.12.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.12.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.12.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.12.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.12.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.12.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.12.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.12.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.12.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.15.cv1.conv.weight: torch.float32\n",
      "model.model.15.cv1.bn.weight: torch.float32\n",
      "model.model.15.cv1.bn.bias: torch.float32\n",
      "model.model.15.cv2.conv.weight: torch.float32\n",
      "model.model.15.cv2.bn.weight: torch.float32\n",
      "model.model.15.cv2.bn.bias: torch.float32\n",
      "model.model.15.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.15.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.15.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.15.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.15.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.15.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.15.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.15.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.15.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.15.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.15.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.15.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.16.conv.weight: torch.float32\n",
      "model.model.16.bn.weight: torch.float32\n",
      "model.model.16.bn.bias: torch.float32\n",
      "model.model.18.cv1.conv.weight: torch.float32\n",
      "model.model.18.cv1.bn.weight: torch.float32\n",
      "model.model.18.cv1.bn.bias: torch.float32\n",
      "model.model.18.cv2.conv.weight: torch.float32\n",
      "model.model.18.cv2.bn.weight: torch.float32\n",
      "model.model.18.cv2.bn.bias: torch.float32\n",
      "model.model.18.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.18.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.18.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.18.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.18.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.18.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.18.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.18.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.18.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.18.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.18.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.18.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.19.conv.weight: torch.float32\n",
      "model.model.19.bn.weight: torch.float32\n",
      "model.model.19.bn.bias: torch.float32\n",
      "model.model.21.cv1.conv.weight: torch.float32\n",
      "model.model.21.cv1.bn.weight: torch.float32\n",
      "model.model.21.cv1.bn.bias: torch.float32\n",
      "model.model.21.cv2.conv.weight: torch.float32\n",
      "model.model.21.cv2.bn.weight: torch.float32\n",
      "model.model.21.cv2.bn.bias: torch.float32\n",
      "model.model.21.m.0.cv1.conv.weight: torch.float32\n",
      "model.model.21.m.0.cv1.bn.weight: torch.float32\n",
      "model.model.21.m.0.cv1.bn.bias: torch.float32\n",
      "model.model.21.m.0.cv2.conv.weight: torch.float32\n",
      "model.model.21.m.0.cv2.bn.weight: torch.float32\n",
      "model.model.21.m.0.cv2.bn.bias: torch.float32\n",
      "model.model.21.m.1.cv1.conv.weight: torch.float32\n",
      "model.model.21.m.1.cv1.bn.weight: torch.float32\n",
      "model.model.21.m.1.cv1.bn.bias: torch.float32\n",
      "model.model.21.m.1.cv2.conv.weight: torch.float32\n",
      "model.model.21.m.1.cv2.bn.weight: torch.float32\n",
      "model.model.21.m.1.cv2.bn.bias: torch.float32\n",
      "model.model.22.cv2.0.0.conv.weight: torch.float32\n",
      "model.model.22.cv2.0.0.bn.weight: torch.float32\n",
      "model.model.22.cv2.0.0.bn.bias: torch.float32\n",
      "model.model.22.cv2.0.1.conv.weight: torch.float32\n",
      "model.model.22.cv2.0.1.bn.weight: torch.float32\n",
      "model.model.22.cv2.0.1.bn.bias: torch.float32\n",
      "model.model.22.cv2.0.2.weight: torch.float32\n",
      "model.model.22.cv2.0.2.bias: torch.float32\n",
      "model.model.22.cv2.1.0.conv.weight: torch.float32\n",
      "model.model.22.cv2.1.0.bn.weight: torch.float32\n",
      "model.model.22.cv2.1.0.bn.bias: torch.float32\n",
      "model.model.22.cv2.1.1.conv.weight: torch.float32\n",
      "model.model.22.cv2.1.1.bn.weight: torch.float32\n",
      "model.model.22.cv2.1.1.bn.bias: torch.float32\n",
      "model.model.22.cv2.1.2.weight: torch.float32\n",
      "model.model.22.cv2.1.2.bias: torch.float32\n",
      "model.model.22.cv2.2.0.conv.weight: torch.float32\n",
      "model.model.22.cv2.2.0.bn.weight: torch.float32\n",
      "model.model.22.cv2.2.0.bn.bias: torch.float32\n",
      "model.model.22.cv2.2.1.conv.weight: torch.float32\n",
      "model.model.22.cv2.2.1.bn.weight: torch.float32\n",
      "model.model.22.cv2.2.1.bn.bias: torch.float32\n",
      "model.model.22.cv2.2.2.weight: torch.float32\n",
      "model.model.22.cv2.2.2.bias: torch.float32\n",
      "model.model.22.cv3.0.0.conv.weight: torch.float32\n",
      "model.model.22.cv3.0.0.bn.weight: torch.float32\n",
      "model.model.22.cv3.0.0.bn.bias: torch.float32\n",
      "model.model.22.cv3.0.1.conv.weight: torch.float32\n",
      "model.model.22.cv3.0.1.bn.weight: torch.float32\n",
      "model.model.22.cv3.0.1.bn.bias: torch.float32\n",
      "model.model.22.cv3.0.2.weight: torch.float32\n",
      "model.model.22.cv3.0.2.bias: torch.float32\n",
      "model.model.22.cv3.1.0.conv.weight: torch.float32\n",
      "model.model.22.cv3.1.0.bn.weight: torch.float32\n",
      "model.model.22.cv3.1.0.bn.bias: torch.float32\n",
      "model.model.22.cv3.1.1.conv.weight: torch.float32\n",
      "model.model.22.cv3.1.1.bn.weight: torch.float32\n",
      "model.model.22.cv3.1.1.bn.bias: torch.float32\n",
      "model.model.22.cv3.1.2.weight: torch.float32\n",
      "model.model.22.cv3.1.2.bias: torch.float32\n",
      "model.model.22.cv3.2.0.conv.weight: torch.float32\n",
      "model.model.22.cv3.2.0.bn.weight: torch.float32\n",
      "model.model.22.cv3.2.0.bn.bias: torch.float32\n",
      "model.model.22.cv3.2.1.conv.weight: torch.float32\n",
      "model.model.22.cv3.2.1.bn.weight: torch.float32\n",
      "model.model.22.cv3.2.1.bn.bias: torch.float32\n",
      "model.model.22.cv3.2.2.weight: torch.float32\n",
      "model.model.22.cv3.2.2.bias: torch.float32\n",
      "model.model.22.dfl.conv.weight: torch.float32\n",
      "Ultralytics 8.3.117  Python-3.10.16 torch-2.5.1 CPU (AMD Ryzen 7 8845HS w/ Radeon 780M Graphics)\n",
      "Model summary (fused): 92 layers, 25,840,918 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\trained_yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (49.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.7s, saved as 'models\\trained_yolov8m.onnx' (98.8 MB)\n",
      "\n",
      "Export complete (3.6s)\n",
      "Results saved to \u001b[1mC:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\trained_yolov8m.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=models\\trained_yolov8m.onnx imgsz=640 data=datasets\\yolo_CropOrWeed2\\data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "\n",
      "Exported model saved to: models\\trained_yolov8m.onnx\n",
      "Model size: 98.81 MB\n",
      "\n",
      "Converting to half precision...\n",
      "Exported model saved to: models\\fp16_yolov8m.onnx\n",
      "Model size: 49.44 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "model = YOLO(\"models\\\\trained_yolov8m.pt\") \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# See the model architecture\n",
    "model.model.info()\n",
    "\n",
    "# checking the models parameters and their data types\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "\n",
    "# Exporting the model to ONNX format\n",
    "exported = model.export(format=\"onnx\")\n",
    "print(f\"\\nExported model saved to: {exported}\")\n",
    "size_mb = os.path.getsize(f\"{exported}\") / (1024 * 1024)\n",
    "print(f\"Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "# using half precision\n",
    "model_fp32 = onnx.load(\"models\\\\trained_yolov8m.onnx\")\n",
    "print(\"\\nConverting to half precision...\")\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, \"models\\\\fp16_yolov8m.onnx\")\n",
    "print(f\"Exported model saved to: models\\\\fp16_yolov8m.onnx\")\n",
    "size_mb = os.path.getsize(\"models\\\\fp16_yolov8m.onnx\") / (1024 * 1024)\n",
    "print(f\"Model size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a771c7a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'models\\\\trained_yolov11n.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m size_mb \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrained_yolov11n.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP32 model size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m size_mb \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfp16_yolov11n.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\genericpath.py:50\u001b[0m, in \u001b[0;36mgetsize\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetsize\u001b[39m(filename):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_size\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'models\\\\trained_yolov11n.onnx'"
     ]
    }
   ],
   "source": [
    "size_mb = os.path.getsize(\"models\\\\trained_yolov11n.onnx\") / (1024 * 1024)\n",
    "print(f\"FP32 model size: {size_mb:.2f} MB\")\n",
    "\n",
    "size_mb = os.path.getsize(\"models\\\\fp16_yolov11n.onnx\") / (1024 * 1024)\n",
    "print(f\"\\n FP16 model size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c277ecd",
   "metadata": {},
   "source": [
    "### Integer quantization\n",
    "Following the steps of https://medium.com/@sulavstha007/quantizing-yolo-v8-models-34c39a2c10e2 static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a24755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Quantization currently not done\n",
    "\n",
    "# to preprocess the model\n",
    "!python -m onnxruntime.quantization.preprocess --input \"models\\\\trained_yolov8m.onnx\" --output \"models\\preprocessed.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from onnxruntime.quantization import CalibrationDataReader, quantize_static, QuantType, QuantFormat\n",
    "    \n",
    "# Class for Callibration Data reading\n",
    "class ImageCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.idx = 0\n",
    "        self.input_name = \"images\"\n",
    "\n",
    "    def get_next(self):\n",
    "        # method to iterate through the data set\n",
    "        if self.idx >= len(self.image_paths):\n",
    "            return None\n",
    "\n",
    "        image_path = self.image_paths[self.idx]\n",
    "        input_data = self.preprocess(image_path)\n",
    "        self.idx += 1\n",
    "        return {self.input_name: input_data}\n",
    "\n",
    "# Assuming you have a list of image paths for calibration\n",
    "calibration_image_paths = ['datasets\\yolo_CropOrWeed2\\images\\val\\ave-0035-0002.jpg',\"datasets\\yolo_CropOrWeed2\\images\\val\\ave-0035-0006.jpg\",\"datasets\\yolo_CropOrWeed2\\images\\val\\ave-0035-0007.jpg\"] # you can add more of the image paths\n",
    "\n",
    "# Create an instance of the ImageCalibrationDataReader\n",
    "calibration_data_reader = ImageCalibrationDataReader(calibration_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the calibration_data_reader with quantize_static\n",
    "# TODO which nodes to exclude?\n",
    "quantize_static('preprocessed.onnx', \"static_quantized.onnx\",\n",
    "                weight_type=QuantType.QInt8,\n",
    "                activation_type=QuantType.QUInt8,\n",
    "                calibration_data_reader=calibration_data_reader,\n",
    "                quant_format=QuantFormat.QDQ,\n",
    "                nodes_to_exclude=['/model.22/Concat_3', '/model.22/Split', '/model.22/Sigmoid'\n",
    "                                 '/model.22/dfl/Reshape', '/model.22/dfl/Transpose', '/model.22/dfl/Softmax', \n",
    "                                 '/model.22/dfl/conv/Conv', '/model.22/dfl/Reshape_1', '/model.22/Slice_1',\n",
    "                                 '/model.22/Slice', '/model.22/Add_1', '/model.22/Sub', '/model.22/Div_1',\n",
    "                                  '/model.22/Concat_4', '/model.22/Mul_2', '/model.22/Concat_5'],\n",
    "                per_channel=False,\n",
    "                reduce_range=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de39c5",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d04340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0045-0012.jpg: 384x640 14 Crops, 11 Weeds, 49.7ms\n",
      "image 2/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0047-0003.jpg: 384x640 15 Weeds, 10.8ms\n",
      "image 3/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0047-0017.jpg: 384x640 23 Weeds, 12.1ms\n",
      "image 4/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0083-0005.jpg: 384x640 1 Crop, 1 Weed, 10.0ms\n",
      "image 5/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0083-0030.jpg: 384x640 2 Crops, 2 Weeds, 10.6ms\n",
      "image 6/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0105-0018.jpg: 384x640 2 Crops, 16 Weeds, 11.7ms\n",
      "image 7/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0127-0019.jpg: 384x640 2 Crops, 17 Weeds, 10.5ms\n",
      "image 8/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0130-0004.jpg: 384x640 3 Crops, 2 Weeds, 11.3ms\n",
      "image 9/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0130-0006.jpg: 384x640 3 Crops, 2 Weeds, 8.2ms\n",
      "image 10/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0006.jpg: 384x640 4 Crops, 7.9ms\n",
      "image 11/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0009.jpg: 384x640 2 Crops, 11.1ms\n",
      "image 12/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0018.jpg: 384x640 2 Crops, 9.7ms\n",
      "image 13/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0014.jpg: 384x640 1 Crop, 1 Weed, 10.0ms\n",
      "image 14/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0015.jpg: 384x640 1 Crop, 10.9ms\n",
      "image 15/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0016.jpg: 384x640 3 Crops, 10.0ms\n",
      "image 16/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0154-0004.jpg: 384x640 3 Crops, 10.0ms\n",
      "image 17/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0303-0007.jpg: 384x640 1 Weed, 11.9ms\n",
      "image 18/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0304-0012.jpg: 384x640 22 Weeds, 9.9ms\n",
      "image 19/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0406-0004.jpg: 384x640 4 Crops, 10.0ms\n",
      "image 20/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0408-0005.jpg: 384x640 7 Weeds, 11.0ms\n",
      "image 21/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0416-0006.jpg: 384x640 23 Weeds, 9.0ms\n",
      "image 22/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0425-0008.jpg: 384x640 19 Weeds, 13.6ms\n",
      "image 23/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0428-0014.jpg: 384x640 23 Weeds, 12.3ms\n",
      "image 24/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0436-0010.jpg: 384x640 10 Weeds, 10.0ms\n",
      "image 25/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0476-0002.jpg: 384x640 2 Crops, 39 Weeds, 14.8ms\n",
      "image 26/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0477-0008.jpg: 384x640 6 Weeds, 11.0ms\n",
      "image 27/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0477-0016.jpg: 384x640 4 Weeds, 11.1ms\n",
      "image 28/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0547-0002.jpg: 384x640 12 Weeds, 10.3ms\n",
      "image 29/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0547-0011.jpg: 384x640 16 Weeds, 11.2ms\n",
      "image 30/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0554-0011.jpg: 384x640 1 Crop, 10.5ms\n",
      "image 31/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0586-0047.jpg: 384x640 1 Crop, 14 Weeds, 10.8ms\n",
      "image 32/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0586-0054.jpg: 384x640 1 Crop, 4 Weeds, 6.1ms\n",
      "image 33/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0587-0005.jpg: 384x640 5 Crops, 2 Weeds, 12.7ms\n",
      "image 34/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0603-0024.jpg: 384x640 6 Weeds, 13.1ms\n",
      "image 35/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0608-0012.jpg: 384x640 6 Crops, 1 Weed, 6.7ms\n",
      "image 36/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0608-0025.jpg: 384x640 4 Crops, 6 Weeds, 8.4ms\n",
      "image 37/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0636-0009.jpg: 384x640 3 Crops, 2 Weeds, 8.1ms\n",
      "image 38/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0639-0019.jpg: 384x640 3 Crops, 12.9ms\n",
      "image 39/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0639-0020.jpg: 384x640 4 Crops, 1 Weed, 7.3ms\n",
      "image 40/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0653-0020.jpg: 384x640 2 Crops, 14 Weeds, 12.7ms\n",
      "image 41/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0654-0003.jpg: 384x640 7 Crops, 28 Weeds, 10.7ms\n",
      "image 42/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0655-0021.jpg: 384x640 2 Crops, 14 Weeds, 13.6ms\n",
      "image 43/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0687-0004.jpg: 384x640 2 Crops, 17 Weeds, 12.8ms\n",
      "image 44/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0687-0019.jpg: 384x640 2 Crops, 7 Weeds, 10.9ms\n",
      "image 45/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0688-0005.jpg: 384x640 4 Crops, 28 Weeds, 6.1ms\n",
      "image 46/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0712-0013.jpg: 384x640 3 Crops, 11.8ms\n",
      "image 47/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0712-0022.jpg: 384x640 2 Crops, 1 Weed, 9.9ms\n",
      "image 48/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0778-0011.jpg: 384x640 19 Weeds, 8.3ms\n",
      "image 49/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0786-0016.jpg: 384x640 15 Weeds, 16.9ms\n",
      "image 50/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0792-0016.jpg: 384x640 1 Crop, 14 Weeds, 12.0ms\n",
      "image 51/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0793-0019.jpg: 384x640 14 Weeds, 13.9ms\n",
      "image 52/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0801-0019.jpg: 384x640 1 Crop, 13 Weeds, 9.2ms\n",
      "image 53/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0803-0013.jpg: 384x640 5 Crops, 34 Weeds, 6.6ms\n",
      "image 54/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0829-0014.jpg: 384x640 6 Crops, 50 Weeds, 0.0ms\n",
      "image 55/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0830-0011.jpg: 384x640 6 Crops, 14 Weeds, 10.6ms\n",
      "image 56/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0832-0003.jpg: 384x640 4 Crops, 7 Weeds, 13.7ms\n",
      "image 57/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0839-0019.jpg: 384x640 4 Weeds, 13.5ms\n",
      "image 58/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0841-0003.jpg: 384x640 16 Weeds, 7.0ms\n",
      "image 59/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0842-0016.jpg: 384x640 1 Weed, 1.6ms\n",
      "image 60/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0854-0016.jpg: 384x640 3 Weeds, 9.1ms\n",
      "image 61/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0854-0017.jpg: 384x640 7 Weeds, 6.4ms\n",
      "image 62/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0855-0013.jpg: 384x640 5 Weeds, 4.8ms\n",
      "image 63/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0866-0005.jpg: 384x640 4 Crops, 1 Weed, 4.5ms\n",
      "image 64/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0867-0008.jpg: 384x640 3 Crops, 3.5ms\n",
      "image 65/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0867-0009.jpg: 384x640 3 Crops, 11.0ms\n",
      "image 66/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0914-0006.jpg: 384x640 15 Weeds, 7.6ms\n",
      "image 67/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0920-0011.jpg: 384x640 20 Weeds, 7.3ms\n",
      "image 68/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0926-0006.jpg: 384x640 4 Crops, 4 Weeds, 8.8ms\n",
      "image 69/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0961-0017.jpg: 384x640 5 Crops, 3 Weeds, 6.1ms\n",
      "image 70/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0967-0007.jpg: 384x640 5 Weeds, 6.9ms\n",
      "image 71/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1114-0006.jpg: 384x640 3 Weeds, 10.0ms\n",
      "image 72/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1114-0009.jpg: 384x640 4 Weeds, 7.3ms\n",
      "image 73/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1116-0006.jpg: 384x640 3 Weeds, 4.4ms\n",
      "image 74/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1218-0014.jpg: 384x640 1 Crop, 0.0ms\n",
      "image 75/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1220-0007.jpg: 384x640 2 Weeds, 6.6ms\n",
      "image 76/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1224-0002.jpg: 384x640 1 Crop, 7.1ms\n",
      "image 77/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1225-0009.jpg: 384x640 2 Weeds, 0.9ms\n",
      "image 78/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1225-0011.jpg: 384x640 5 Weeds, 14.0ms\n",
      "image 79/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1231-0019.jpg: 384x640 1 Crop, 15.2ms\n",
      "image 80/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1232-0008.jpg: 384x640 2 Crops, 17.6ms\n",
      "image 81/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1233-0005.jpg: 384x640 13 Weeds, 10.7ms\n",
      "image 82/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1236-0011.jpg: 384x640 2 Crops, 3 Weeds, 4.9ms\n",
      "image 83/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1237-0007.jpg: 384x640 7 Weeds, 10.6ms\n",
      "image 84/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1237-0011.jpg: 384x640 1 Crop, 4 Weeds, 8.2ms\n",
      "image 85/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1247-0002.jpg: 384x640 4 Weeds, 6.4ms\n",
      "image 86/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1247-0004.jpg: 384x640 1 Weed, 7.5ms\n",
      "image 87/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1248-0002.jpg: 384x640 5 Weeds, 10.6ms\n",
      "image 88/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1256-0019.jpg: 384x640 2 Crops, 9.9ms\n",
      "image 89/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1256-0021.jpg: 384x640 2 Crops, 12.7ms\n",
      "image 90/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1258-0007.jpg: 384x640 1 Crop, 5.3ms\n",
      "image 91/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1265-0007.jpg: 384x640 1 Crop, 12 Weeds, 3.0ms\n",
      "image 92/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1265-0015.jpg: 384x640 4 Crops, 3 Weeds, 9.4ms\n",
      "image 93/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1266-0002.jpg: 384x640 1 Crop, 12 Weeds, 5.0ms\n",
      "image 94/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0011.jpg: 384x640 6 Weeds, 5.5ms\n",
      "image 95/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0015.jpg: 384x640 7 Weeds, 2.6ms\n",
      "image 96/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0016.jpg: 384x640 9 Weeds, 10.9ms\n",
      "image 97/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1283-0007.jpg: 384x640 3 Crops, 0.0ms\n",
      "image 98/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1284-0004.jpg: 384x640 2 Crops, 5.4ms\n",
      "image 99/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1284-0007.jpg: 384x640 2 Crops, 5.0ms\n",
      "image 100/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1291-0003.jpg: 384x640 5 Crops, 15 Weeds, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\pseudo_labels\u001b[0m\n",
      "100 labels saved to runs\\detect\\pseudo_labels\\labels\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "model = YOLO(\"models\\\\trained_yolov8m.pt\") \n",
    "\n",
    "\n",
    "# Run inference on training set to create pseudo-labels\n",
    "results = model.predict(\n",
    "    source=\"datasets\\\\100_images\",  # path to folder with images\n",
    "    save=False,                    # saves images with predictions (optional)\n",
    "    save_txt=True,                # saves predictions in YOLO format (.txt)\n",
    "    save_conf=True,               # includes confidence score       \n",
    "    name=\"pseudo_labels\",        # output folder: runs/detect/pseudo_labels\n",
    "    exist_ok=True                 # overwrite if already exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_detection_loss(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute the detection loss for YOLO models.\n",
    "    :param predictions: Output from the student model.\n",
    "    :param targets: Ground truth labels.\n",
    "    :return: Total detection loss.\n",
    "    \"\"\"\n",
    "    # Example: Combine classification, objectness, and bounding box losses\n",
    "    cls_loss = F.cross_entropy(predictions['cls_logits'], targets['cls_labels'])\n",
    "    obj_loss = F.binary_cross_entropy(predictions['obj_scores'], targets['obj_scores'])\n",
    "    bbox_loss = F.mse_loss(predictions['bbox_coords'], targets['bbox_coords'])\n",
    "\n",
    "    total_loss = cls_loss + obj_loss + bbox_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd359589",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Teacher output (no gradient tracking)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 32\u001b[0m     teacher_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mteacher_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     teacher_logits \u001b[38;5;241m=\u001b[39m teacher_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_logits\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Adjust index as needed\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Student output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:51\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and extract the models\n",
    "teacher_wrapper = YOLO('models\\\\trained_yolov8m.pt') # teacher model\n",
    "teacher_model = teacher_wrapper.model\n",
    "teacher_model.eval() \n",
    "\n",
    "student_wrapper = YOLO('models\\custom_model.yaml') # student model\n",
    "student_model = student_wrapper.model\n",
    "student_model.train()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(annotations_dir='datasets\\yolo_CropOrWeed2\\labels\\\\train', img_dir='datasets\\yolo_CropOrWeed2\\images\\\\train', transform=None)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=CustomDataset.collate_fn)\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "T = 2.0           # Temperature for softening logits\n",
    "lambda_kd = 0.5   # Weight for the distillation loss\n",
    "\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "for images, labels in data_loader:\n",
    "    images = images.to('cuda') \n",
    "    labels = labels.to('cuda')\n",
    "    \n",
    "    # Teacher output (no gradient tracking)\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(images)\n",
    "        teacher_logits = teacher_outputs['cls_logits']  # Adjust index as needed\n",
    "    \n",
    "    # Student output\n",
    "    student_outputs = student_model(images)\n",
    "    student_logits = student_outputs['cls_logits']  # Adjust based on your model structure \n",
    "\n",
    "    # Apply temperature scaling\n",
    "    teacher_soft = F.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_soft = F.log_softmax(student_logits / T, dim=1)\n",
    "    \n",
    "    # Compute the distillation loss (KL-divergence)\n",
    "    loss_kd = F.kl_div(student_log_soft, teacher_soft, reduction='batchmean') * (T * T)\n",
    "    \n",
    "    # Compute the standard detection loss (your custom YOLO loss function)\n",
    "    # loss_detection = compute_detection_loss(student_outputs, labels) \n",
    "    loss_detection = compute_detection_loss(student_outputs, labels)  # TODO see if works\n",
    "\n",
    "    # Combine the losses\n",
    "    loss_total = loss_detection + lambda_kd * loss_kd\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Detection Loss: {loss_detection.item():.4f} | KD Loss: {loss_kd.item():.4f} | Total Loss: {loss_total.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc6b35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m student_out \u001b[38;5;241m=\u001b[39m student(imgs)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Flatten class logits\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m teacher_flat \u001b[38;5;241m=\u001b[39m \u001b[43mextract_cls_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m student_flat \u001b[38;5;241m=\u001b[39m extract_cls_flat(student_out)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Compute distillation loss (KL divergence)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mextract_cls_flat\u001b[1;34m(model_outputs, head_index)\u001b[0m\n\u001b[0;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m preds[head_index]        \u001b[38;5;66;03m# e.g. tensor of shape (B, A, H, W, 5+nc)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m cls_logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m5\u001b[39m:]      \u001b[38;5;66;03m# drop box coords & objectness → (B,A,H,W,C)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m B, A, H, W, C \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     43\u001b[0m flat \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m A \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W, C)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flat\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load teacher and student models\n",
    "device = 'cuda'\n",
    "teacher = YOLO('models/trained_yolov8m.pt').model.to(device).eval()\n",
    "student = YOLO('models/custom_model.yaml').model.to(device).train()\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = CustomDataset(\n",
    "    annotations_dir='datasets\\yolo_100_images\\labels',\n",
    "    img_dir='datasets\\yolo_100_images\\images',\n",
    "    transform=None\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=CustomDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Distillation hyperparameters\n",
    "T = 2.0                      # Softmax temperature\n",
    "lambda_kd = 0.5              # Distillation loss weight\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "# Detection loss using Ultralytics implementation\n",
    "def compute_detection_loss(model, imgs, labels):\n",
    "    preds = model(imgs)\n",
    "    loss, _ = model.loss(preds, labels)\n",
    "    return loss\n",
    "\n",
    "# Extract class logits from a single head and flatten to [N, C]\n",
    "def extract_cls_flat(model_outputs, head_index=-1):\n",
    "    # model_outputs may be (preds_list, features)\n",
    "    preds = model_outputs[0] if isinstance(model_outputs, tuple) else model_outputs\n",
    "    out = preds[head_index]        # e.g. tensor of shape (B, A, H, W, 5+nc)\n",
    "    cls_logits = out[..., 5:]      # drop box coords & objectness → (B,A,H,W,C)\n",
    "    B, A, H, W, C = cls_logits.shape\n",
    "    flat = cls_logits.reshape(B * A * H * W, C)\n",
    "    return flat\n",
    "\n",
    "# Training loop\n",
    "for imgs, labels in loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels = [lb.to(device) for lb in labels]\n",
    "\n",
    "    # Teacher forward\n",
    "    with torch.no_grad():\n",
    "        teacher_out = teacher(imgs)\n",
    "    # Student forward\n",
    "    student_out = student(imgs)\n",
    "\n",
    "    # Flatten class logits\n",
    "    teacher_flat = extract_cls_flat(teacher_out)\n",
    "    student_flat = extract_cls_flat(student_out)\n",
    "\n",
    "    # Compute distillation loss (KL divergence)\n",
    "    t = T\n",
    "    log_p_s = F.log_softmax(student_flat / t, dim=1)\n",
    "    p_t = F.softmax(teacher_flat / t, dim=1)\n",
    "    loss_kd = F.kl_div(log_p_s, p_t, reduction='batchmean') * (t * t)\n",
    "\n",
    "    # Compute detection loss\n",
    "    loss_det = compute_detection_loss(student, imgs, labels)\n",
    "\n",
    "    # Combine losses\n",
    "    loss = loss_det + lambda_kd * loss_kd\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Det Loss: {loss_det.item():.4f} | KD Loss: {loss_kd.item():.4f} | Total: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b47eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 66, -1]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m s_feats \u001b[38;5;241m=\u001b[39m stud_feat_layer(imgs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Detection loss via wrapper\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m loss_det \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_detection_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Feature distillation loss\u001b[39;00m\n\u001b[0;32m     51\u001b[0m loss_feat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(s_feats, t_feats)\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36mcompute_detection_loss\u001b[1;34m(wrapper, imgs, labels)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_detection_loss\u001b[39m(wrapper, imgs, labels):\n\u001b[0;32m     27\u001b[0m     preds \u001b[38;5;241m=\u001b[39m wrapper(imgs)               \u001b[38;5;66;03m# wrapper returns (preds, features)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:295\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m    294\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\loss.py:210\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    208\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n\u001b[0;32m    209\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 210\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(feats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m feats], \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m    211\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    214\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    215\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\loss.py:210\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n\u001b[0;32m    209\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 210\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m feats], \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m    211\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    214\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    215\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[5, 66, -1]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load teacher and student as Ultralytics YOLO wrappers\n",
    "device = 'cuda'\n",
    "teacher = YOLO('models/trained_yolov8m.pt').model.to(device).eval()\n",
    "student = YOLO('models/custom_model.yaml').model.to(device).train()\n",
    "\n",
    "# DataLoader\n",
    "dataset = CustomDataset(\n",
    "    annotations_dir='datasets\\yolo_100_images\\labels',\n",
    "    img_dir='datasets\\yolo_100_images\\images',\n",
    "    transform=None\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=CustomDataset.collate_fn)\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_feat = 0.5  # feature distillation weight\n",
    "default_lr = 1e-4\n",
    "optimizer = torch.optim.Adam(student.model.parameters(), lr=default_lr)\n",
    "\n",
    "# Detection loss using wrapper\n",
    "def compute_detection_loss(wrapper, imgs, labels):\n",
    "    preds = wrapper(imgs)               # wrapper returns (preds, features)\n",
    "    loss, _ = wrapper.loss(preds, labels)\n",
    "    return loss\n",
    "\n",
    "# Feature extractor layers\n",
    "teach_feat_layer = teacher.model[:3]  # first modules\n",
    "stud_feat_layer  = student.model[:3]\n",
    "\n",
    "# Training loop\n",
    "for imgs, labels in loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels = [l.to(device) for l in labels]\n",
    "\n",
    "    # Teacher features\n",
    "    with torch.no_grad():\n",
    "        t_feats = teach_feat_layer(imgs)\n",
    "\n",
    "    # Student features\n",
    "    s_feats = stud_feat_layer(imgs)\n",
    "\n",
    "    # Detection loss via wrapper\n",
    "    loss_det = compute_detection_loss(student, imgs, labels)\n",
    "\n",
    "    # Feature distillation loss\n",
    "    loss_feat = F.mse_loss(s_feats, t_feats)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = loss_det + lambda_feat * loss_feat\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Det Loss: {loss_det.item():.4f} | Feat Loss: {loss_feat.item():.4f} | Total: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddda2d",
   "metadata": {},
   "source": [
    "## Pruning (graph based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO, __version__\n",
    "from ultralytics.nn.modules import Detect, C2f, Conv, Bottleneck, C2fCIB, RepVGGDW, Concat, PSA, Attention, C3k2, C3k, CIB\n",
    "from ultralytics.nn.tasks import attempt_load_one_weight\n",
    "from ultralytics.engine.model import TASK2DATA\n",
    "from ultralytics.engine.trainer import BaseTrainer\n",
    "from ultralytics.utils import yaml_load, LOGGER, RANK, DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS\n",
    "from ultralytics.utils.checks import check_yaml\n",
    "from ultralytics.utils.torch_utils import initialize_weights, de_parallel\n",
    "\n",
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39e7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def save_pruning_performance_graph(x, y1, y11, y2, y3, pruning_method=\"L2\"):\n",
    "    \"\"\"\n",
    "    Draw performance change graph\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : List\n",
    "        Parameter numbers of all pruning steps\n",
    "    y1 : List\n",
    "        mAPs after fine-tuning of all pruning steps\n",
    "    y11 : List\n",
    "        mAP50 after fine-tuning of all pruning steps\n",
    "    y2 : List\n",
    "        MACs of all pruning steps\n",
    "    y3 : List\n",
    "        mAPs after pruning (not fine-tuned) of all pruning steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.style.use(\"ggplot\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    x, y1, y11, y2, y3 = np.array(x), np.array(y1), np.array(y11), np.array(y2), np.array(y3)\n",
    "    y2_ratio = y2 / y2[0]\n",
    "\n",
    "    # create the figure and the axis object\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # plot the pruned mAP and recovered mAP\n",
    "    ax.set_xlabel('Pruning Ratio')\n",
    "    ax.set_ylabel('mAP')\n",
    "    ax.plot(x, y1, label='recovered mAP')\n",
    "    ax.scatter(x, y1)\n",
    "    ax.plot(x, y11, color='tab:blue', label='recovered mAP50')\n",
    "    ax.scatter(x, y11, color='tab:blue')\n",
    "    ax.plot(x, y3, color='tab:gray', label='pruned mAP')\n",
    "    ax.scatter(x, y3, color='tab:gray')\n",
    "\n",
    "    # create a second axis that shares the same x-axis\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    # plot the second set of data\n",
    "    ax2.set_ylabel('MACs')\n",
    "    ax2.plot(x, y2_ratio, color='tab:orange', label='MACs')\n",
    "    ax2.scatter(x, y2_ratio, color='tab:orange')\n",
    "\n",
    "    # add a legend\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    ax.set_xlim(105, 20)\n",
    "    ax.set_ylim(0, max(y11) + 0.05)\n",
    "    ax2.set_ylim(0.05, 1.05)\n",
    "\n",
    "    # calculate the highest and lowest points for each set of data\n",
    "    max_y1_idx = np.argmax(y1)\n",
    "    min_y1_idx = np.argmin(y1)\n",
    "    max_y11_idx = np.argmax(y11)\n",
    "    min_y11_idx = np.argmin(y11)\n",
    "    max_y2_idx = np.argmax(y2)\n",
    "    min_y2_idx = np.argmin(y2)\n",
    "    max_y1 = y1[max_y1_idx]\n",
    "    min_y1 = y1[min_y1_idx]\n",
    "    max_y11 = y11[max_y11_idx]\n",
    "    min_y11 = y11[min_y11_idx]\n",
    "    max_y2 = y2_ratio[max_y2_idx]\n",
    "    min_y2 = y2_ratio[min_y2_idx]\n",
    "\n",
    "    # add text for the highest and lowest values near the points\n",
    "    ax.text(x[max_y1_idx], max_y1 - 0.05, f'max mAP = {max_y1:.2f}', fontsize=10)\n",
    "    ax.text(x[min_y1_idx], min_y1 + 0.02, f'min mAP = {min_y1:.2f}', fontsize=10)\n",
    "\n",
    "    ax.text(x[max_y11_idx], max_y11 + 0.02, f'max mAP50 = {max_y11:.2f}', fontsize=10)\n",
    "    ax.text(x[min_y11_idx], min_y11 + 0.02, f'min mAP50 = {min_y11:.2f}', fontsize=10)\n",
    "    \n",
    "    ax2.text(x[max_y2_idx], max_y2 - 0.05, f'max MACs = {max_y2 * y2[0] / 1e9:.2f}G', fontsize=10)\n",
    "    ax2.text(x[min_y2_idx], min_y2 + 0.02, f'min MACs = {min_y2 * y2[0] / 1e9:.2f}G', fontsize=10)\n",
    "\n",
    "    plt.title('Comparison of mAP / mAP50 and MACs with Pruning Ratio')\n",
    "    plt.savefig(f'pruning_perf_change_{pruning_method}.png')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "def infer_c3k2_shortcut(bottleneck):\n",
    "    \"\"\"\n",
    "    Infer whether to use shortcut and large-kernel flag from a child block.\n",
    "    Returns (shortcut: bool, is_cib: bool, lk: bool).\n",
    "    \"\"\"\n",
    "    # Bottleneck case: identical logic to C2f shortcut\n",
    "    if isinstance(bottleneck, Bottleneck):\n",
    "        c1 = bottleneck.cv1.conv.in_channels\n",
    "        c2 = bottleneck.cv2.conv.out_channels\n",
    "        add_flag = getattr(bottleneck, 'add', False)\n",
    "        return (c1 == c2 and add_flag), False, False\n",
    "    # C3k case: preserve shortcut and detect large-kernel usage\n",
    "    elif isinstance(bottleneck, C3k):\n",
    "        lk = any(isinstance(mod, RepVGGDW) for mod in getattr(bottleneck, 'm', []))\n",
    "        add_flag = getattr(bottleneck, 'add', False)\n",
    "        return add_flag, False, lk\n",
    "    # Fallback: treat as C3k-like\n",
    "    else:\n",
    "        lk = any(isinstance(mod, RepVGGDW) for mod in getattr(bottleneck, 'm', []))\n",
    "        add_flag = getattr(bottleneck, 'add', False)\n",
    "        return add_flag, False, lk\n",
    "\n",
    "class C2f_v2(nn.Module):\n",
    "    # CSP Bottleneck with 2 convolutions\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5,is_CIB = False, lk = False):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv0 = Conv(c1, self.c, 1, 1)\n",
    "        self.cv1 = Conv(c1, self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        if not is_CIB:\n",
    "            self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "        else:\n",
    "            self.m = nn.ModuleList(CIB(self.c, self.c, shortcut, e=1.0, lk=lk) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y = list(self.cv1(x).chunk(2, 1))\n",
    "        y = [self.cv0(x), self.cv1(x)]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "class C3k2_v2(C2f_v2):\n",
    "    \"\"\"\n",
    "    Torch-pruning-compatible version of C3k2.\n",
    "    Splits the original cv1 into cv0/cv1 and rebuilds its module list.\n",
    "    \"\"\"\n",
    "    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        # initialize using C2f_v2 scaffold\n",
    "        super().__init__(c1, c2, n, shortcut, g, e, is_CIB=False, lk=False)\n",
    "        # override the m list with C3k or Bottleneck blocks\n",
    "        self.m = nn.ModuleList(\n",
    "            C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g)\n",
    "            for _ in range(n)\n",
    "        )\n",
    "\n",
    "\n",
    "def transfer_weights_c3k2(src: C3k2, dst: C3k2_v2):\n",
    "    \"\"\"\n",
    "    Transfer parameters and buffers from original C3k2 to C3k2_v2.\n",
    "    \"\"\"\n",
    "    # reuse final conv and module list pointers\n",
    "    dst.cv2 = src.cv2\n",
    "    dst.m = src.m\n",
    "\n",
    "    state_src = src.state_dict()\n",
    "    state_dst = dst.state_dict()\n",
    "\n",
    "    # split cv1 weights and BN buffers into cv0 and cv1\n",
    "    w_old = state_src['cv1.conv.weight']\n",
    "    half = w_old.shape[0] // 2\n",
    "    state_dst['cv0.conv.weight'] = w_old[:half]\n",
    "    state_dst['cv1.conv.weight'] = w_old[half:]\n",
    "    for bn in ['weight', 'bias', 'running_mean', 'running_var']:\n",
    "        v = state_src[f'cv1.bn.{bn}']\n",
    "        state_dst[f'cv0.bn.{bn}'] = v[:half]\n",
    "        state_dst[f'cv1.bn.{bn}'] = v[half:]\n",
    "\n",
    "    # copy all other parameters and buffers\n",
    "    for key, val in state_src.items():\n",
    "        if not key.startswith('cv1.'):\n",
    "            state_dst[key] = val\n",
    "\n",
    "    # copy non-callable attributes\n",
    "    for attr in dir(src):\n",
    "        if not callable(getattr(src, attr)) and '_' not in attr:\n",
    "            setattr(dst, attr, getattr(src, attr))\n",
    "\n",
    "    dst.load_state_dict(state_dst)\n",
    "\n",
    "\n",
    "def replace_c3k2_with_v2(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Recursively replace all C3k2 instances in `module` with C3k2_v2.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, C3k2):\n",
    "            # infer flags from first inner block\n",
    "            c3k_flag = isinstance(child.m[0], C3k)\n",
    "            shortcut, is_cib, lk = infer_c3k2_shortcut(child.m[0])\n",
    "            # instantiate new block\n",
    "            v2 = C3k2_v2(\n",
    "                child.cv1.conv.in_channels,\n",
    "                child.cv2.conv.out_channels,\n",
    "                n=len(child.m),\n",
    "                c3k=c3k_flag,\n",
    "                e=child.c / child.cv2.conv.out_channels,\n",
    "                g=(child.m[0].cv2.conv.groups if not is_cib else child.cv2.conv.groups),\n",
    "                shortcut=shortcut,\n",
    "            )\n",
    "            # transfer weights and replace\n",
    "            transfer_weights_c3k2(child, v2)\n",
    "            setattr(module, name, v2)\n",
    "        else:\n",
    "            replace_c3k2_with_v2(child)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "def save_model_v2(self: BaseTrainer):\n",
    "    \"\"\"\n",
    "    Disabled half precision saving. originated from ultralytics/yolo/engine/trainer.py\n",
    "    \"\"\"\n",
    "    ckpt = {\n",
    "        'epoch': self.epoch,\n",
    "        'best_fitness': self.best_fitness,\n",
    "        'model': deepcopy(de_parallel(self.model)),\n",
    "        'ema': deepcopy(self.ema.ema),\n",
    "        'updates': self.ema.updates,\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "        'train_args': vars(self.args),  # save as dict\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'version': __version__}\n",
    "\n",
    "    # Save last, best and delete\n",
    "    torch.save(ckpt, self.last)\n",
    "    if self.best_fitness == self.fitness:\n",
    "        torch.save(ckpt, self.best)\n",
    "    if (self.epoch > 0) and (self.save_period > 0) and (self.epoch % self.save_period == 0):\n",
    "        torch.save(ckpt, self.wdir / f'epoch{self.epoch}.pt')\n",
    "    del ckpt\n",
    "\n",
    "\n",
    "def final_eval_v2(self: BaseTrainer):\n",
    "    \"\"\"\n",
    "    originated from ultralytics/yolo/engine/trainer.py\n",
    "    \"\"\"\n",
    "    for f in self.last, self.best:\n",
    "        if f.exists():\n",
    "            strip_optimizer_v2(f)  # strip optimizers\n",
    "            if f is self.best:\n",
    "                LOGGER.info(f'\\nValidating {f}...')\n",
    "                self.metrics = self.validator(model=f)\n",
    "                self.metrics.pop('fitness', None)\n",
    "                self.run_callbacks('on_fit_epoch_end')\n",
    "\n",
    "\n",
    "def strip_optimizer_v2(f: Union[str, Path] = 'best.pt', s: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Disabled half precision saving. originated from ultralytics/yolo/utils/torch_utils.py\n",
    "    \"\"\"\n",
    "    x = torch.load(f, map_location=torch.device('cpu'))\n",
    "    args = {**DEFAULT_CFG_DICT, **x['train_args']}  # combine model args with default args, preferring model args\n",
    "    if x.get('ema'):\n",
    "        x['model'] = x['ema']  # replace model with ema\n",
    "    for k in 'optimizer', 'ema', 'updates':  # keys\n",
    "        x[k] = None\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    x['train_args'] = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # strip non-default keys\n",
    "    # x['model'].args = x['train_args']\n",
    "    torch.save(x, s or f)\n",
    "    mb = os.path.getsize(s or f) / 1E6  # filesize\n",
    "    LOGGER.info(f\"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB\")\n",
    "\n",
    "\n",
    "def train_v2(self: YOLO, **train_params):\n",
    "    \"\"\"\n",
    "    Disabled loading new model when pruning flag is set. originated from ultralytics/yolo/engine/model.py\n",
    "    \"\"\"\n",
    "    \n",
    "    self._check_is_pytorch_model()\n",
    "    # Override Training Parameters with provided ones \n",
    "    overrides = self.overrides.copy()\n",
    "    overrides.update(train_params)\n",
    "    if train_params.get('cfg'):\n",
    "        overrides = yaml_load(check_yaml(train_params['cfg']))\n",
    "    overrides['mode'] = 'train'\n",
    "    if not overrides.get('data'):\n",
    "        raise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\n",
    "    if overrides.get('resume'):\n",
    "        overrides['resume'] = self.ckpt_path\n",
    "\n",
    "    # Initialize trainer\n",
    "    self.task = \"detect\"\n",
    "    self.callbacks = []\n",
    "    self.trainer = self.task_map[self.task]['trainer'](overrides=overrides, _callbacks=self.callbacks)\n",
    "\n",
    "    self.trainer.verbose = False  \n",
    "\n",
    "    # pruning mode\n",
    "    self.trainer.pruning = True\n",
    "    self.trainer.model = self.model\n",
    "    \n",
    "    # replace some functions to disable half precision saving\n",
    "    self.trainer.save_model = save_model_v2.__get__(self.trainer)\n",
    "    self.trainer.final_eval = final_eval_v2.__get__(self.trainer)\n",
    "\n",
    "    self.trainer.train()\n",
    "    # Update model and cfg after training\n",
    "    if RANK in (-1, 0):\n",
    "        self.model, _ = attempt_load_one_weight(str(self.trainer.best))\n",
    "        self.overrides = self.model.args\n",
    "        self.metrics = getattr(self.trainer.validator, 'metrics', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, target_prune_rate=0.8, iterations=24, map_threshold=0.10, train_params=\"finetune.yaml\"):\n",
    "    \"\"\"\n",
    "    Apply structured channel pruning to the model.\n",
    "    :param model: The YOLO model to prune.\n",
    "    :param amount: The fraction of channels to prune.\n",
    "    \"\"\"\n",
    "    # set training to train_v2 and load the training parameters\n",
    "    model.__setattr__(\"train_v2\", train_v2.__get__(model))\n",
    "    train_params = yaml_load(check_yaml(train_params))\n",
    "    # Set the model to training mode\n",
    "    model.model.train()  \n",
    "\n",
    "    # change c2f implementation to be compatible with the Graph pruner\n",
    "    replace_c3k2_with_v2(model.model) # avoids shared references\n",
    "    initialize_weights(model.model)\n",
    "\n",
    "    # unfreeze all the layers, making them trainable\n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = True # set all to True\n",
    "    # dummy input to trace  the model's computation graph\n",
    "    example_inputs = torch.randn(1, 3, train_params[\"imgsz\"], train_params[\"imgsz\"]).to(model.device)\n",
    "    \n",
    "    # Initialize metrics list to plot and logging purpose\n",
    "    macs_list, nparams_list, map_list, map50_list, pruned_map_list = [], [], [], [], []\n",
    "    \n",
    "    # Get the initial number of FLOPs and parameters\n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs)\n",
    "    \n",
    "    # Do validation before pruning model for baseline metrics\n",
    "    train_params['name'] = \"baseline_val\"\n",
    "    train_params['batch'] = 1\n",
    "    validation_model = deepcopy(model)\n",
    "    results = validation_model.val(**train_params, verbose=False)\n",
    "    # Save the metrics\n",
    "    init_map = results.box.map\n",
    "    init_map50 = results.box.map50\n",
    "    \n",
    "    # add the initial metrics to the lists\n",
    "    macs_list.append(base_macs)\n",
    "    nparams_list.append(100) # as % of parameters\n",
    "    map_list.append(init_map)\n",
    "    map50_list.append(init_map50)\n",
    "    pruned_map_list.append(init_map)\n",
    "    print(f\"Before Pruning: MACs={base_macs / 1e9: .5f} G, #Params={base_nparams / 1e6: .5f} M, mAP={init_map: .5f}\")\n",
    "    \n",
    "    # prune same ratio of filter based on initial size\n",
    "    prune_per_step = target_prune_rate / iterations\n",
    "\n",
    "    # Iterate and prune based on the selected iterative steps\n",
    "    print(f\"\\n -------------- MODEL PRUNING ----------------\")\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Reset the loss function and unfreeze the layers\n",
    "        model.model.criterion = None \n",
    "        model.model.train()\n",
    "        for name, param in model.model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Filter out the layers that don't want to be pruned\n",
    "        ignored_layers = []\n",
    "        unwrapped_parameters = []\n",
    "        for m in model.model.modules():\n",
    "            if isinstance(m, (Detect,Attention,Concat,C3k2)):\n",
    "                ignored_layers.append(m)\n",
    "        \n",
    "        example_inputs = example_inputs.to(model.device) # move dummy input to device\n",
    "        \n",
    "        # Initialize the pruner instance (structured pruning on channels or filters)\n",
    "        pruner = tp.pruner.GroupNormPruner(\n",
    "            model.model,\n",
    "            example_inputs,\n",
    "            importance=tp.importance.GroupMagnitudeImportance(p=2),  # L2 norm pruning\n",
    "            iterative_steps=1,\n",
    "            pruning_ratio=prune_per_step,\n",
    "            ignored_layers=ignored_layers,\n",
    "            unwrapped_parameters=unwrapped_parameters\n",
    "        )\n",
    "                \n",
    "        # prune the model\n",
    "        pruner.step()\n",
    "\n",
    "        print(f\"\\n--------------- STEP {i + 1} OF {iterations} ----------------\")\n",
    "        print(f\"\\n---------- PRUNED RATIO : {1- (1 - prune_per_step)**(i+1):.3f} -----------\")\n",
    "        \n",
    "        # Validation after the model been pruned - before fine-tuning\n",
    "        train_params['name'] = f\"step_{i}_pre_val\" \n",
    "        train_params['batch'] = 1\n",
    "        validation_model.model = deepcopy(model.model)\n",
    "        metric = validation_model.val(**train_params, verbose=False)\n",
    "        pruned_map = metric.box.map\n",
    "        pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(pruner.model, example_inputs.to(model.device))\n",
    "        current_speed_up = float(macs_list[0]) / pruned_macs\n",
    "        \n",
    "        print(f\"After pruning iter {i + 1}: MACs={pruned_macs / 1e9} G, #Params={pruned_nparams / 1e6} M, \"\n",
    "            f\"mAP={pruned_map}, speed up={current_speed_up},  pruned_param_ratio={pruned_nparams / base_nparams * 100:.3f} %\")\n",
    "        \n",
    "        # fine-tuning the pruned model\n",
    "        model.model.train()\n",
    "        for name, param in model.model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        train_params['name'] = f\"step_{i}_finetune\"\n",
    "        train_params['batch'] = -1\n",
    "        model.train_v2(**train_params)\n",
    "        \n",
    "        # post fine-tuning validation\n",
    "        train_params['name'] = f\"step_{i}_post_val\"\n",
    "        train_params['batch'] = 1\n",
    "        validation_model = YOLO(model.trainer.best)\n",
    "        metric = validation_model.val(**train_params,verbose=False)\n",
    "        current_map = metric.box.map\n",
    "        current_map50 = metric.box.map50\n",
    "        print(f\"\\nAfter fine tuning mAP={current_map} - mAP50={current_map50} - pruned_param_ratio={pruned_nparams / base_nparams * 100:.3f}% \\n\")\n",
    "        \n",
    "        macs_list.append(pruned_macs)\n",
    "        nparams_list.append(pruned_nparams / base_nparams * 100)\n",
    "        pruned_map_list.append(pruned_map)\n",
    "        map_list.append(current_map)\n",
    "        map50_list.append(current_map50)\n",
    "        \n",
    "        # remove pruner after single iteration\n",
    "        del pruner\n",
    "        \n",
    "        save_pruning_performance_graph(nparams_list, map_list, map50_list, macs_list, pruned_map_list, \"L2 norm\")\n",
    "        \n",
    "        # Stop if the metrics drop is greater than the max_map_drop parameter\n",
    "        if init_map - current_map > map_threshold:\n",
    "            print(\"Pruning early stop\")\n",
    "            break\n",
    "\n",
    "    # --- End Iterative Pruning Loop ---\n",
    "    # Combine all metrics into a dictionary\n",
    "    metrics = {\n",
    "        \"macs_list\": macs_list,\n",
    "        \"nparams_list\": nparams_list,\n",
    "        \"pruned_map_list\": pruned_map_list,\n",
    "        \"map_list\": map_list,\n",
    "        \"map50_list\": map50_list\n",
    "    }\n",
    "    with open(\"pruning_metrics.json\", 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    # Export final model\n",
    "    exported = model.export(format=\"onnx\")\n",
    "    print(f\"Final pruned model exported to ONNX, {exported}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eca7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\yolo_CropOrWeed2\\labels\\val.cache... 1541 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1541/1541 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrained_yolov11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mWARNING)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mprune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_prune_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetune.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m, in \u001b[0;36mprune_model\u001b[1;34m(model, target_prune_rate, iterations, map_threshold, train_params)\u001b[0m\n\u001b[0;32m     31\u001b[0m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m validation_model \u001b[38;5;241m=\u001b[39m deepcopy(model)\n\u001b[1;32m---> 33\u001b[0m results \u001b[38;5;241m=\u001b[39m validation_model\u001b[38;5;241m.\u001b[39mval(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_params, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Save the metrics\u001b[39;00m\n\u001b[0;32m     35\u001b[0m init_map \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\engine\\model.py:627\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    626\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 627\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\engine\\validator.py:193\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msplit), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m    192\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 193\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchannels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_val_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m dt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    197\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    198\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    199\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    200\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    201\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:811\u001b[0m, in \u001b[0;36mAutoBackend.warmup\u001b[1;34m(self, imgsz)\u001b[0m\n\u001b[0;32m    809\u001b[0m im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mimgsz, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:595\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 595\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:159\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    160\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:76\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 76\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:119\u001b[0m, in \u001b[0;36mDetect._inference\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    117\u001b[0m x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x], \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape):\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmake_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgetpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfjs\u001b[39m\u001b[38;5;124m\"\u001b[39m}:  \u001b[38;5;66;03m# avoid TF FlexSplitV ops\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\tal.py:375\u001b[0m, in \u001b[0;36mmake_anchors\u001b[1;34m(feats, strides, grid_cell_offset)\u001b[0m\n\u001b[0;32m    373\u001b[0m     sy, sx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_10 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx)\n\u001b[0;32m    374\u001b[0m     anchor_points\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack((sx, sy), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m--> 375\u001b[0m     stride_tensor\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(anchor_points), torch\u001b[38;5;241m.\u001b[39mcat(stride_tensor)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"models\\\\trained_yolov11n.pt\")\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "\n",
    "prune_model(model, target_prune_rate=0.8, iterations=5, map_threshold=0.10, train_params=\"finetune.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb15a31",
   "metadata": {},
   "source": [
    "# non graph based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (2): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (4): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Conv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (6): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): Conv(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (8): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SPPF(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (10): C2PSA(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): PSABlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): Conv(\n",
      "              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "            (proj): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "            (pe): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (ffn): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (12): Concat()\n",
      "    (13): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (15): Concat()\n",
      "    (16): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): Conv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (18): Concat()\n",
      "    (19): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): Conv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (21): Concat()\n",
      "    (22): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): Detect(\n",
      "      (cv2): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (cv3): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (dfl): DFL(\n",
      "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"models\\\\trained_yolov11n.pt\")\n",
    "\n",
    "torch_model = model.model\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf517d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def prune_model(model, amount=0.1):\n",
    "    for module in model.model.modules():\n",
    "            # prune Conv2d layers and BatchNorm2d layers\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                prune.ln_structured(module, name=\"weight\", amount=amount, n=1, dim=0) # l1 norm for convolutional layers\n",
    "                prune.remove(module, \"weight\")  # Remove reparam so zeros become actual weights\n",
    "            '''\n",
    "                next_module = next(module.model.modules()) # TODO does this really get the respective layer?\n",
    "                if isinstance(next_module, nn.BatchNorm2d):\n",
    "                    # Prune BatchNorm γ (scaling factor)\n",
    "                    prune.ln_structured(next_module, name=\"weight\", amount=amount, n=1, dim=0)\n",
    "                    prune.remove(next_module, \"weight\")\n",
    "                    # prune BatchNorm β (bias)\n",
    "                    prune.ln_structured(next_module, name=\"bias\", amount=amount, n=1, dim=0)\n",
    "                    prune.remove(next_module, \"bias\")\n",
    "            # prune Linear layers\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                prune.l1_unstructured(module, name=\"weight\", amount=amount / 3)  # l1 norm unstructured pruning TODO check amount\n",
    "                prune.remove(module, \"weight\")\n",
    "            '''\n",
    "    return model\n",
    "\n",
    "def iterative_pruning(model, target_prune_rate=0.8, iterations=24, map_threshold=0.10):\n",
    "    \"\"\"\n",
    "    Perform iterative pruning on the model.\n",
    "    :param model: The YOLO model to prune.\n",
    "    :param target_prune_rate: Total fraction of weights/channels to prune.\n",
    "    :param iterations: Number of pruning steps.\n",
    "    :param map_threshold: Minimum acceptable accuracy (mAP50-95) to stop pruning.\n",
    "    :return: The pruned model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch_model = model.model\n",
    "\n",
    "    # Evaluate before pruning\n",
    "    results = model.val(data=\"datasets\\\\yolo_CropOrWeed2\\\\data.yaml\", device=device)  \n",
    "    initial_map = results.box.map\n",
    "    print(f\"Initial mAP50-95: {initial_map:.4f}\")\n",
    "    print(f\"Model size before pruning: {sum(p.numel() for p in torch_model.parameters()) / 1e6:.2f}M parameters, {sum(p.element_size() * p.numel() for p in torch_model.parameters()) / (1024 * 1024):.2f} MB\")\n",
    "    print(torch_model)\n",
    "\n",
    "    prune_per_step = target_prune_rate / iterations\n",
    "\n",
    "    #TODO iterate\n",
    "    print(\"Pruning model...\")\n",
    "    torch_pruned_model = prune_model(torch_model, amount=0.1)  # Prune the model\n",
    "    print(\"Model pruned.\")\n",
    "    print(f\"Model size after pruning: {sum(p.numel() for p in torch_pruned_model.parameters()) / 1e6:.2f}M parameters, {sum(p.element_size() * p.numel() for p in torch_pruned_model.parameters()) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    model.model = torch_pruned_model ## Update the model in the YOLO wrapper\n",
    "    # retrain and evaluate\n",
    "    model.train(\n",
    "        data=\"datasets\\\\yolo_CropOrWeed2\\\\data.yaml\",\n",
    "        epochs=50,\n",
    "        imgsz=224,\n",
    "        batch=-1,\n",
    "        lr0=0.001,\n",
    "        lrf=0.1\n",
    "    )\n",
    "    results = model.val(data=\"datasets\\\\yolo_CropOrWeed2\\\\data.yaml\", device=device)  # Evaluate after pruning\n",
    "    print(f\"mAP50-95: {results.box.map}\") # TODO store the results in a list and plot them later\n",
    "    # TODO stop when hitting accuracy threshold\n",
    "\n",
    "    # TODO strip out the zeroed channels save and export the pruned model\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"models\\\\trained_yolov11n.pt\")\n",
    "target_prune_rate = 0.8\n",
    "iterative_steps = 24\n",
    "map_threshold = 0.10\n",
    "model = iterative_pruning(model, target_prune_rate, iterations=iterative_steps, map_threshold=map_threshold)\n",
    "print(\"model trained, saving...\")\n",
    "model.save(\"models\\\\pruned_yolov11n.pt\")  # Save the pruned model after training\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Evaluate the pruned model\n",
    "results = model.val(data=\"datasets\\\\yolo_CropOrWeed2\\\\data.yaml\", device=device)  # Evaluate after pruning\n",
    "print(f\"mAP50-95: {results.box.map}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
