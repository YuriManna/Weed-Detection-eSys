{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94c8bb8",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0045-0012.jpg: 384x640 14 Crops, 11 Weeds, 49.7ms\n",
      "image 2/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0047-0003.jpg: 384x640 15 Weeds, 10.8ms\n",
      "image 3/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0047-0017.jpg: 384x640 23 Weeds, 12.1ms\n",
      "image 4/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0083-0005.jpg: 384x640 1 Crop, 1 Weed, 10.0ms\n",
      "image 5/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0083-0030.jpg: 384x640 2 Crops, 2 Weeds, 10.6ms\n",
      "image 6/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0105-0018.jpg: 384x640 2 Crops, 16 Weeds, 11.7ms\n",
      "image 7/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0127-0019.jpg: 384x640 2 Crops, 17 Weeds, 10.5ms\n",
      "image 8/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0130-0004.jpg: 384x640 3 Crops, 2 Weeds, 11.3ms\n",
      "image 9/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0130-0006.jpg: 384x640 3 Crops, 2 Weeds, 8.2ms\n",
      "image 10/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0006.jpg: 384x640 4 Crops, 7.9ms\n",
      "image 11/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0009.jpg: 384x640 2 Crops, 11.1ms\n",
      "image 12/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0140-0018.jpg: 384x640 2 Crops, 9.7ms\n",
      "image 13/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0014.jpg: 384x640 1 Crop, 1 Weed, 10.0ms\n",
      "image 14/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0015.jpg: 384x640 1 Crop, 10.9ms\n",
      "image 15/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0153-0016.jpg: 384x640 3 Crops, 10.0ms\n",
      "image 16/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\ave-0154-0004.jpg: 384x640 3 Crops, 10.0ms\n",
      "image 17/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0303-0007.jpg: 384x640 1 Weed, 11.9ms\n",
      "image 18/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0304-0012.jpg: 384x640 22 Weeds, 9.9ms\n",
      "image 19/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0406-0004.jpg: 384x640 4 Crops, 10.0ms\n",
      "image 20/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0408-0005.jpg: 384x640 7 Weeds, 11.0ms\n",
      "image 21/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0416-0006.jpg: 384x640 23 Weeds, 9.0ms\n",
      "image 22/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0425-0008.jpg: 384x640 19 Weeds, 13.6ms\n",
      "image 23/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0428-0014.jpg: 384x640 23 Weeds, 12.3ms\n",
      "image 24/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0436-0010.jpg: 384x640 10 Weeds, 10.0ms\n",
      "image 25/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0476-0002.jpg: 384x640 2 Crops, 39 Weeds, 14.8ms\n",
      "image 26/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0477-0008.jpg: 384x640 6 Weeds, 11.0ms\n",
      "image 27/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0477-0016.jpg: 384x640 4 Weeds, 11.1ms\n",
      "image 28/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0547-0002.jpg: 384x640 12 Weeds, 10.3ms\n",
      "image 29/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0547-0011.jpg: 384x640 16 Weeds, 11.2ms\n",
      "image 30/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0554-0011.jpg: 384x640 1 Crop, 10.5ms\n",
      "image 31/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0586-0047.jpg: 384x640 1 Crop, 14 Weeds, 10.8ms\n",
      "image 32/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0586-0054.jpg: 384x640 1 Crop, 4 Weeds, 6.1ms\n",
      "image 33/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0587-0005.jpg: 384x640 5 Crops, 2 Weeds, 12.7ms\n",
      "image 34/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0603-0024.jpg: 384x640 6 Weeds, 13.1ms\n",
      "image 35/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0608-0012.jpg: 384x640 6 Crops, 1 Weed, 6.7ms\n",
      "image 36/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0608-0025.jpg: 384x640 4 Crops, 6 Weeds, 8.4ms\n",
      "image 37/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0636-0009.jpg: 384x640 3 Crops, 2 Weeds, 8.1ms\n",
      "image 38/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0639-0019.jpg: 384x640 3 Crops, 12.9ms\n",
      "image 39/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0639-0020.jpg: 384x640 4 Crops, 1 Weed, 7.3ms\n",
      "image 40/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0653-0020.jpg: 384x640 2 Crops, 14 Weeds, 12.7ms\n",
      "image 41/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0654-0003.jpg: 384x640 7 Crops, 28 Weeds, 10.7ms\n",
      "image 42/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0655-0021.jpg: 384x640 2 Crops, 14 Weeds, 13.6ms\n",
      "image 43/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0687-0004.jpg: 384x640 2 Crops, 17 Weeds, 12.8ms\n",
      "image 44/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0687-0019.jpg: 384x640 2 Crops, 7 Weeds, 10.9ms\n",
      "image 45/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0688-0005.jpg: 384x640 4 Crops, 28 Weeds, 6.1ms\n",
      "image 46/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0712-0013.jpg: 384x640 3 Crops, 11.8ms\n",
      "image 47/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0712-0022.jpg: 384x640 2 Crops, 1 Weed, 9.9ms\n",
      "image 48/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0778-0011.jpg: 384x640 19 Weeds, 8.3ms\n",
      "image 49/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0786-0016.jpg: 384x640 15 Weeds, 16.9ms\n",
      "image 50/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0792-0016.jpg: 384x640 1 Crop, 14 Weeds, 12.0ms\n",
      "image 51/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0793-0019.jpg: 384x640 14 Weeds, 13.9ms\n",
      "image 52/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0801-0019.jpg: 384x640 1 Crop, 13 Weeds, 9.2ms\n",
      "image 53/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0803-0013.jpg: 384x640 5 Crops, 34 Weeds, 6.6ms\n",
      "image 54/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0829-0014.jpg: 384x640 6 Crops, 50 Weeds, 0.0ms\n",
      "image 55/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0830-0011.jpg: 384x640 6 Crops, 14 Weeds, 10.6ms\n",
      "image 56/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0832-0003.jpg: 384x640 4 Crops, 7 Weeds, 13.7ms\n",
      "image 57/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0839-0019.jpg: 384x640 4 Weeds, 13.5ms\n",
      "image 58/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0841-0003.jpg: 384x640 16 Weeds, 7.0ms\n",
      "image 59/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0842-0016.jpg: 384x640 1 Weed, 1.6ms\n",
      "image 60/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0854-0016.jpg: 384x640 3 Weeds, 9.1ms\n",
      "image 61/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0854-0017.jpg: 384x640 7 Weeds, 6.4ms\n",
      "image 62/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0855-0013.jpg: 384x640 5 Weeds, 4.8ms\n",
      "image 63/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0866-0005.jpg: 384x640 4 Crops, 1 Weed, 4.5ms\n",
      "image 64/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0867-0008.jpg: 384x640 3 Crops, 3.5ms\n",
      "image 65/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0867-0009.jpg: 384x640 3 Crops, 11.0ms\n",
      "image 66/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0914-0006.jpg: 384x640 15 Weeds, 7.6ms\n",
      "image 67/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0920-0011.jpg: 384x640 20 Weeds, 7.3ms\n",
      "image 68/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0926-0006.jpg: 384x640 4 Crops, 4 Weeds, 8.8ms\n",
      "image 69/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0961-0017.jpg: 384x640 5 Crops, 3 Weeds, 6.1ms\n",
      "image 70/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-0967-0007.jpg: 384x640 5 Weeds, 6.9ms\n",
      "image 71/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1114-0006.jpg: 384x640 3 Weeds, 10.0ms\n",
      "image 72/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1114-0009.jpg: 384x640 4 Weeds, 7.3ms\n",
      "image 73/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1116-0006.jpg: 384x640 3 Weeds, 4.4ms\n",
      "image 74/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1218-0014.jpg: 384x640 1 Crop, 0.0ms\n",
      "image 75/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1220-0007.jpg: 384x640 2 Weeds, 6.6ms\n",
      "image 76/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1224-0002.jpg: 384x640 1 Crop, 7.1ms\n",
      "image 77/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1225-0009.jpg: 384x640 2 Weeds, 0.9ms\n",
      "image 78/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1225-0011.jpg: 384x640 5 Weeds, 14.0ms\n",
      "image 79/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1231-0019.jpg: 384x640 1 Crop, 15.2ms\n",
      "image 80/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1232-0008.jpg: 384x640 2 Crops, 17.6ms\n",
      "image 81/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1233-0005.jpg: 384x640 13 Weeds, 10.7ms\n",
      "image 82/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1236-0011.jpg: 384x640 2 Crops, 3 Weeds, 4.9ms\n",
      "image 83/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1237-0007.jpg: 384x640 7 Weeds, 10.6ms\n",
      "image 84/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1237-0011.jpg: 384x640 1 Crop, 4 Weeds, 8.2ms\n",
      "image 85/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1247-0002.jpg: 384x640 4 Weeds, 6.4ms\n",
      "image 86/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1247-0004.jpg: 384x640 1 Weed, 7.5ms\n",
      "image 87/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1248-0002.jpg: 384x640 5 Weeds, 10.6ms\n",
      "image 88/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1256-0019.jpg: 384x640 2 Crops, 9.9ms\n",
      "image 89/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1256-0021.jpg: 384x640 2 Crops, 12.7ms\n",
      "image 90/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1258-0007.jpg: 384x640 1 Crop, 5.3ms\n",
      "image 91/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1265-0007.jpg: 384x640 1 Crop, 12 Weeds, 3.0ms\n",
      "image 92/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1265-0015.jpg: 384x640 4 Crops, 3 Weeds, 9.4ms\n",
      "image 93/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1266-0002.jpg: 384x640 1 Crop, 12 Weeds, 5.0ms\n",
      "image 94/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0011.jpg: 384x640 6 Weeds, 5.5ms\n",
      "image 95/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0015.jpg: 384x640 7 Weeds, 2.6ms\n",
      "image 96/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1275-0016.jpg: 384x640 9 Weeds, 10.9ms\n",
      "image 97/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1283-0007.jpg: 384x640 3 Crops, 0.0ms\n",
      "image 98/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1284-0004.jpg: 384x640 2 Crops, 5.4ms\n",
      "image 99/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1284-0007.jpg: 384x640 2 Crops, 5.0ms\n",
      "image 100/100 c:\\Users\\yurim\\Documents\\University\\UM\\Year_3\\Bachelor_Thesis\\Weed_Detection_eSys\\datasets\\100_images\\vwg-1291-0003.jpg: 384x640 5 Crops, 15 Weeds, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\pseudo_labels\u001b[0m\n",
      "100 labels saved to runs\\detect\\pseudo_labels\\labels\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "model = YOLO(\"models\\\\trained_yolov8m.pt\") \n",
    "\n",
    "\n",
    "# Run inference on training set to create pseudo-labels\n",
    "results = model.predict(\n",
    "    source=\"datasets\\\\100_images\",  # path to folder with images\n",
    "    save=False,                    # saves images with predictions (optional)\n",
    "    save_txt=True,                # saves predictions in YOLO format (.txt)\n",
    "    save_conf=True,               # includes confidence score       \n",
    "    name=\"pseudo_labels\",        # output folder: runs/detect/pseudo_labels\n",
    "    exist_ok=True                 # overwrite if already exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_detection_loss(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute the detection loss for YOLO models.\n",
    "    :param predictions: Output from the student model.\n",
    "    :param targets: Ground truth labels.\n",
    "    :return: Total detection loss.\n",
    "    \"\"\"\n",
    "    # Example: Combine classification, objectness, and bounding box losses\n",
    "    cls_loss = F.cross_entropy(predictions['cls_logits'], targets['cls_labels'])\n",
    "    obj_loss = F.binary_cross_entropy(predictions['obj_scores'], targets['obj_scores'])\n",
    "    bbox_loss = F.mse_loss(predictions['bbox_coords'], targets['bbox_coords'])\n",
    "\n",
    "    total_loss = cls_loss + obj_loss + bbox_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb147e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n",
      "\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Teacher output (no gradient tracking)\u001b[39;00m\n",
      "\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;32m---> 32\u001b[0m     teacher_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mteacher_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     33\u001b[0m     teacher_logits \u001b[38;5;241m=\u001b[39m teacher_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_logits\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Adjust index as needed\u001b[39;00m\n",
      "\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Student output\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n",
      "\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n",
      "\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n",
      "\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n",
      "\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n",
      "\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n",
      "\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n",
      "\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n",
      "\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:51\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n",
      "\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n",
      "\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n",
      "\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n",
      "\u001b[0;32m    548\u001b[0m     )\n",
      "\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n",
      "\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and extract the models\n",
    "teacher_wrapper = YOLO('models\\\\trained_yolov8m.pt') # teacher model\n",
    "teacher_model = teacher_wrapper.model\n",
    "teacher_model.eval() \n",
    "\n",
    "student_wrapper = YOLO('models\\custom_model.yaml') # student model\n",
    "student_model = student_wrapper.model\n",
    "student_model.train()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(annotations_dir='datasets\\yolo_CropOrWeed2\\labels\\\\train', img_dir='datasets\\yolo_CropOrWeed2\\images\\\\train', transform=None)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=CustomDataset.collate_fn)\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "T = 2.0           # Temperature for softening logits\n",
    "lambda_kd = 0.5   # Weight for the distillation loss\n",
    "\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "for images, labels in data_loader:\n",
    "    images = images.to('cuda') \n",
    "    labels = labels.to('cuda')\n",
    "    \n",
    "    # Teacher output (no gradient tracking)\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(images)\n",
    "        teacher_logits = teacher_outputs['cls_logits']  # Adjust index as needed\n",
    "    \n",
    "    # Student output\n",
    "    student_outputs = student_model(images)\n",
    "    student_logits = student_outputs['cls_logits']  # Adjust based on your model structure \n",
    "\n",
    "    # Apply temperature scaling\n",
    "    teacher_soft = F.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_soft = F.log_softmax(student_logits / T, dim=1)\n",
    "    \n",
    "    # Compute the distillation loss (KL-divergence)\n",
    "    loss_kd = F.kl_div(student_log_soft, teacher_soft, reduction='batchmean') * (T * T)\n",
    "    \n",
    "    # Compute the standard detection loss (your custom YOLO loss function)\n",
    "    # loss_detection = compute_detection_loss(student_outputs, labels) \n",
    "    loss_detection = compute_detection_loss(student_outputs, labels)  # TODO see if works\n",
    "\n",
    "    # Combine the losses\n",
    "    loss_total = loss_detection + lambda_kd * loss_kd\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Detection Loss: {loss_detection.item():.4f} | KD Loss: {loss_kd.item():.4f} | Total Loss: {loss_total.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a33dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n",
      "\u001b[0;32m     55\u001b[0m student_out \u001b[38;5;241m=\u001b[39m student(imgs)\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Flatten class logits\u001b[39;00m\n",
      "\u001b[1;32m---> 58\u001b[0m teacher_flat \u001b[38;5;241m=\u001b[39m \u001b[43mextract_cls_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_out\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     59\u001b[0m student_flat \u001b[38;5;241m=\u001b[39m extract_cls_flat(student_out)\n",
      "\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Compute distillation loss (KL divergence)\u001b[39;00m\n",
      "\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mextract_cls_flat\u001b[1;34m(model_outputs, head_index)\u001b[0m\n",
      "\u001b[0;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m preds[head_index]        \u001b[38;5;66;03m# e.g. tensor of shape (B, A, H, W, 5+nc)\u001b[39;00m\n",
      "\u001b[0;32m     41\u001b[0m cls_logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m5\u001b[39m:]      \u001b[38;5;66;03m# drop box coords & objectness → (B,A,H,W,C)\u001b[39;00m\n",
      "\u001b[1;32m---> 42\u001b[0m B, A, H, W, C \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;32m     43\u001b[0m flat \u001b[38;5;241m=\u001b[39m cls_logits\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m A \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W, C)\n",
      "\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flat\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load teacher and student models\n",
    "device = 'cuda'\n",
    "teacher = YOLO('models/trained_yolov8m.pt').model.to(device).eval()\n",
    "student = YOLO('models/custom_model.yaml').model.to(device).train()\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = CustomDataset(\n",
    "    annotations_dir='datasets\\yolo_100_images\\labels',\n",
    "    img_dir='datasets\\yolo_100_images\\images',\n",
    "    transform=None\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=CustomDataset.collate_fn\n",
    ")\n",
    "\n",
    "# Distillation hyperparameters\n",
    "T = 2.0                      # Softmax temperature\n",
    "lambda_kd = 0.5              # Distillation loss weight\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "# Detection loss using Ultralytics implementation\n",
    "def compute_detection_loss(model, imgs, labels):\n",
    "    preds = model(imgs)\n",
    "    loss, _ = model.loss(preds, labels)\n",
    "    return loss\n",
    "\n",
    "# Extract class logits from a single head and flatten to [N, C]\n",
    "def extract_cls_flat(model_outputs, head_index=-1):\n",
    "    # model_outputs may be (preds_list, features)\n",
    "    preds = model_outputs[0] if isinstance(model_outputs, tuple) else model_outputs\n",
    "    out = preds[head_index]        # e.g. tensor of shape (B, A, H, W, 5+nc)\n",
    "    cls_logits = out[..., 5:]      # drop box coords & objectness → (B,A,H,W,C)\n",
    "    B, A, H, W, C = cls_logits.shape\n",
    "    flat = cls_logits.reshape(B * A * H * W, C)\n",
    "    return flat\n",
    "\n",
    "# Training loop\n",
    "for imgs, labels in loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels = [lb.to(device) for lb in labels]\n",
    "\n",
    "    # Teacher forward\n",
    "    with torch.no_grad():\n",
    "        teacher_out = teacher(imgs)\n",
    "    # Student forward\n",
    "    student_out = student(imgs)\n",
    "\n",
    "    # Flatten class logits\n",
    "    teacher_flat = extract_cls_flat(teacher_out)\n",
    "    student_flat = extract_cls_flat(student_out)\n",
    "\n",
    "    # Compute distillation loss (KL divergence)\n",
    "    t = T\n",
    "    log_p_s = F.log_softmax(student_flat / t, dim=1)\n",
    "    p_t = F.softmax(teacher_flat / t, dim=1)\n",
    "    loss_kd = F.kl_div(log_p_s, p_t, reduction='batchmean') * (t * t)\n",
    "\n",
    "    # Compute detection loss\n",
    "    loss_det = compute_detection_loss(student, imgs, labels)\n",
    "\n",
    "    # Combine losses\n",
    "    loss = loss_det + lambda_kd * loss_kd\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Det Loss: {loss_det.item():.4f} | KD Loss: {loss_kd.item():.4f} | Total: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9913c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 66, -1]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n",
      "\u001b[0;32m     45\u001b[0m s_feats \u001b[38;5;241m=\u001b[39m stud_feat_layer(imgs)\n",
      "\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Detection loss via wrapper\u001b[39;00m\n",
      "\u001b[1;32m---> 48\u001b[0m loss_det \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_detection_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Feature distillation loss\u001b[39;00m\n",
      "\u001b[0;32m     51\u001b[0m loss_feat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(s_feats, t_feats)\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36mcompute_detection_loss\u001b[1;34m(wrapper, imgs, labels)\u001b[0m\n",
      "\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_detection_loss\u001b[39m(wrapper, imgs, labels):\n",
      "\u001b[0;32m     27\u001b[0m     preds \u001b[38;5;241m=\u001b[39m wrapper(imgs)               \u001b[38;5;66;03m# wrapper returns (preds, features)\u001b[39;00m\n",
      "\u001b[1;32m---> 28\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\nn\\tasks.py:295\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n",
      "\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n",
      "\u001b[0;32m    294\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n",
      "\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\loss.py:210\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n",
      "\u001b[0;32m    208\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n",
      "\u001b[0;32m    209\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n",
      "\u001b[1;32m--> 210\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(feats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m feats], \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n",
      "\u001b[0;32m    211\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;32m    212\u001b[0m )\n",
      "\u001b[0;32m    214\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;32m    215\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\yurim\\anaconda3\\envs\\cuda\\lib\\site-packages\\ultralytics\\utils\\loss.py:210\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    208\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n",
      "\u001b[0;32m    209\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n",
      "\u001b[1;32m--> 210\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m feats], \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n",
      "\u001b[0;32m    211\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;32m    212\u001b[0m )\n",
      "\u001b[0;32m    214\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;32m    215\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[5, 66, -1]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from Dataloader import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load teacher and student as Ultralytics YOLO wrappers\n",
    "device = 'cuda'\n",
    "teacher = YOLO('models/trained_yolov8m.pt').model.to(device).eval()\n",
    "student = YOLO('models/custom_model.yaml').model.to(device).train()\n",
    "\n",
    "# DataLoader\n",
    "dataset = CustomDataset(\n",
    "    annotations_dir='datasets\\yolo_100_images\\labels',\n",
    "    img_dir='datasets\\yolo_100_images\\images',\n",
    "    transform=None\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=CustomDataset.collate_fn)\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_feat = 0.5  # feature distillation weight\n",
    "default_lr = 1e-4\n",
    "optimizer = torch.optim.Adam(student.model.parameters(), lr=default_lr)\n",
    "\n",
    "# Detection loss using wrapper\n",
    "def compute_detection_loss(wrapper, imgs, labels):\n",
    "    preds = wrapper(imgs)               # wrapper returns (preds, features)\n",
    "    loss, _ = wrapper.loss(preds, labels)\n",
    "    return loss\n",
    "\n",
    "# Feature extractor layers\n",
    "teach_feat_layer = teacher.model[:3]  # first modules\n",
    "stud_feat_layer  = student.model[:3]\n",
    "\n",
    "# Training loop\n",
    "for imgs, labels in loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels = [l.to(device) for l in labels]\n",
    "\n",
    "    # Teacher features\n",
    "    with torch.no_grad():\n",
    "        t_feats = teach_feat_layer(imgs)\n",
    "\n",
    "    # Student features\n",
    "    s_feats = stud_feat_layer(imgs)\n",
    "\n",
    "    # Detection loss via wrapper\n",
    "    loss_det = compute_detection_loss(student, imgs, labels)\n",
    "\n",
    "    # Feature distillation loss\n",
    "    loss_feat = F.mse_loss(s_feats, t_feats)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = loss_det + lambda_feat * loss_feat\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Det Loss: {loss_det.item():.4f} | Feat Loss: {loss_feat.item():.4f} | Total: {total_loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
